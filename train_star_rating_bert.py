import pandas as pd
import numpy as np
from datasets import Dataset
from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    Trainer,
    TrainingArguments,
)
import joblib
import torch
from transformers import BertModel
from sklearn.utils import shuffle

if __name__ == '__main__':
    import multiprocessing
    multiprocessing.freeze_support()  # å¯æœ‰å¯ç„¡ï¼Œä½†ä¿éšªèµ·è¦‹ä¿ç•™

    # âœ… é¡¯ç¤º CUDA ç‹€æ…‹
    print("CUDA æ˜¯å¦å¯ç”¨:", torch.cuda.is_available())
    print("ä½¿ç”¨è£ç½®:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU")

    # æ¸¬è©¦ BERT æ˜¯å¦èƒ½ä¸Š GPUï¼ˆå¯åˆªï¼‰
    model_test = BertModel.from_pretrained("bert-base-uncased")
    model_test.to("cuda" if torch.cuda.is_available() else "cpu")
    print("æ¨¡å‹æ‰€åœ¨è£ç½®:", next(model_test.parameters()).device)

    # âœ… 1. è¼‰å…¥ CSVï¼Œå‡è¡¡æŠ½æ¨£
    df = pd.read_csv("star_rating_dataset.csv")  # æ‡‰åŒ…å« text èˆ‡ label æ¬„ä½
    label_col = "label"
    unique_labels = sorted(df[label_col].unique())
    # âœ… å°å‡ºæ¯å€‹æ˜Ÿç­‰æœ‰å¤šå°‘ç­†è³‡æ–™
    print("ğŸ“Š æ¯å€‹æ˜Ÿç­‰çš„è³‡æ–™æ•¸é‡ï¼š")
    print(df["label"].value_counts().sort_index())

    sample_per_class = 2000  # âœ… æ¯å€‹é¡åˆ¥æœ€å¤šæŠ½å¹¾ç­†
    balanced_df = (
        df.groupby(label_col)
        .apply(lambda x: x.sample(n=min(len(x), sample_per_class), random_state=42))
        .reset_index(drop=True)
    )

    # âœ… æ‰“äº‚ + åˆ†å‰² train/testï¼ˆ8:2ï¼‰
    balanced_df = shuffle(balanced_df, random_state=42)
    dataset = Dataset.from_pandas(balanced_df)
    dataset = dataset.train_test_split(test_size=0.2)

    # âœ… 2. Tokenizer
    model_name = "distilbert-base-uncased"
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    def tokenize(batch):
        texts = batch["text"]
        texts = [t if isinstance(t, str) else "" for t in texts]
        return tokenizer(texts, truncation=True, padding="max_length", max_length=256)

    tokenized = dataset.map(tokenize, batched=True)

    # âœ… 3. æ¨¡å‹åˆå§‹åŒ–
    num_labels = len(unique_labels)
    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)

    # âœ… 4. è¨“ç·´åƒæ•¸è¨­å®š
    args = TrainingArguments(
        output_dir="./bert_star_rating",
        evaluation_strategy="epoch",
        save_strategy="epoch",
        dataloader_num_workers=3,
        dataloader_pin_memory=True,
        num_train_epochs=10,
        per_device_train_batch_size=32,
        per_device_eval_batch_size=32,
        metric_for_best_model="accuracy"
    )

    # âœ… 5. è©•ä¼°æŒ‡æ¨™
    def compute_metrics(p):
        preds = np.argmax(p.predictions, axis=1)
        labels = p.label_ids
        acc = (preds == labels).mean()
        return {"accuracy": acc}

    # âœ… 6. é–‹å§‹è¨“ç·´
    print("ğŸ§  é–‹å§‹è¨“ç·´ BERT æ¨¡å‹...")
    trainer = Trainer(
        model=model,
        args=args,
        train_dataset=tokenized["train"],
        eval_dataset=tokenized["test"],
        tokenizer=tokenizer,
        compute_metrics=compute_metrics
    )
    trainer.train()

    # âœ… 7. å„²å­˜æ¨¡å‹èˆ‡ tokenizer
    trainer.save_model("bert_star_rating_model")
    tokenizer.save_pretrained("bert_star_rating_model")

    # âœ… 8. å„²å­˜ label mapï¼ˆé æ¸¬æ™‚è¦ç”¨ï¼‰
    label_map = {i: label for i, label in enumerate(unique_labels)}
    joblib.dump(label_map, "bert_label_map.pkl")
